{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "####     //      Import/Install packages/REST API       \\\\       ####\n",
    "# Default packages\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Preinstalled packages\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Define desired wd, where I want to save my .csv files\n",
    "desired_wd = '/Users/jenskoning/Documents/Python_projects/dsr_media_nlp/dsr_media_nlp/Script/'\n",
    "os.chdir(desired_wd)\n",
    "\n",
    "# URL of our News API\n",
    "base_url = 'https://api.newscatcherapi.com/v2/search'\n",
    "\n",
    "# My API key\n",
    "X_API_KEY = 'uPGwd9h_DeqHKvxrDL5z9fu3t1Mc9rroBabtvX0iOuE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "####     //      Query Data       \\\\       ####\n",
    "# Put my API key to headers in order to be authorized to perform a call\n",
    "headers = {'x-api-key': X_API_KEY}\n",
    "\n",
    "params = [\n",
    "    {\n",
    "    'q': 'Digital Silk Road AND DSR',\n",
    "    'lang': 'en',\n",
    "    'to_rank': 10000,\n",
    "    'page_size': 100,\n",
    "    'from': '3 years ago', \n",
    "    'page': 1\n",
    "    },\n",
    "    {\n",
    "    'q': 'Digital Silk Road AND DSR AND China',\n",
    "    'lang': 'en',\n",
    "    'to_rank': 10000,\n",
    "    'page_size': 100,\n",
    "    'from': '3 years ago', \n",
    "    'page': 1\n",
    "    },\n",
    "    {\n",
    "    'q': 'Digital Silk Road AND DSR AND Malaysia',\n",
    "    'lang': 'en',\n",
    "    'to_rank': 10000,\n",
    "    'page_size': 100,\n",
    "    'from': '3 years ago', \n",
    "    'page': 1\n",
    "    },\n",
    "    {\n",
    "    'q': 'Digital Silk Road AND DSR AND Indonesia',\n",
    "    'lang': 'en',\n",
    "    'to_rank': 10000,\n",
    "    'page_size': 100,\n",
    "    'from': '3 years ago', \n",
    "    'page': 1\n",
    "    },\n",
    "    {\n",
    "    'q': 'Digital Silk Road AND DSR AND Philippines',\n",
    "    'lang': 'en',\n",
    "    'to_rank': 10000,\n",
    "    'page_size': 100,\n",
    "    'from': '3 years ago', \n",
    "    'page': 1\n",
    "    },\n",
    "    {\n",
    "    'q': 'Digital Silk Road AND DSR AND Geopolitics',\n",
    "    'lang': 'en',\n",
    "    'to_rank': 10000,\n",
    "    'page_size': 100,\n",
    "    'from': '3 years ago', \n",
    "    'page': 1\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/jenskoning/Documents/Python_projects/dsr_media_nlp/dsr_media_nlp/Script/api_searchmultiple_csv*.ipynb Cell 3\u001b[0m in \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jenskoning/Documents/Python_projects/dsr_media_nlp/dsr_media_nlp/Script/api_searchmultiple_csv%2A.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m####     //      Convert into pandas table       \\\\       ####\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jenskoning/Documents/Python_projects/dsr_media_nlp/dsr_media_nlp/Script/api_searchmultiple_csv%2A.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m pandas_table \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(results[\u001b[39m'\u001b[39m\u001b[39marticles\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jenskoning/Documents/Python_projects/dsr_media_nlp/dsr_media_nlp/Script/api_searchmultiple_csv%2A.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m display(pandas_table)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "####     //      Convert into pandas table       \\\\       ####\n",
    "pandas_table = pd.DataFrame(results['articles'])\n",
    "display(pandas_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query in use => {'q': 'Digital Silk Road AND DSR', 'lang': 'en', 'to_rank': 10000, 'page_size': 100, 'from': '3 years ago', 'page': 1}\n",
      "Done for page number => 1\n",
      "All articles have been extracted\n",
      "Query in use => {'q': 'Digital Silk Road AND DSR AND China', 'lang': 'en', 'to_rank': 10000, 'page_size': 100, 'from': '3 years ago', 'page': 1}\n",
      "Done for page number => 1\n",
      "All articles have been extracted\n",
      "Query in use => {'q': 'Digital Silk Road AND DSR AND Malaysia', 'lang': 'en', 'to_rank': 10000, 'page_size': 100, 'from': '3 years ago', 'page': 1}\n",
      "Done for page number => 1\n",
      "All articles have been extracted\n",
      "Query in use => {'q': 'Digital Silk Road AND DSR AND Indonesia', 'lang': 'en', 'to_rank': 10000, 'page_size': 100, 'from': '3 years ago', 'page': 1}\n",
      "Done for page number => 1\n",
      "All articles have been extracted\n",
      "Query in use => {'q': 'Digital Silk Road AND DSR AND Philippines', 'lang': 'en', 'to_rank': 10000, 'page_size': 100, 'from': '3 years ago', 'page': 1}\n",
      "Done for page number => 1\n",
      "All articles have been extracted\n",
      "Query in use => {'q': 'Digital Silk Road AND DSR AND Geopolitics', 'lang': 'en', 'to_rank': 10000, 'page_size': 100, 'from': '3 years ago', 'page': 1}\n",
      "Done for page number => 1\n",
      "All articles have been extracted\n",
      "Number of extracted articles => 184\n"
     ]
    }
   ],
   "source": [
    "# Variable to store all found news articles, mp stands for \"multiple queries\"\n",
    "all_news_articles_mp = []\n",
    "\n",
    "# Infinite loop which ends when all articles are extracted\n",
    "for separated_param in params:\n",
    "\n",
    "    print(f'Query in use => {str(separated_param)}')\n",
    "    \n",
    "    while True:\n",
    "        # Wait for 1 second between each call\n",
    "        time.sleep(1)\n",
    "\n",
    "        # GET Call from previous section enriched with some logs\n",
    "        response = requests.get(base_url, headers=headers, params=separated_param)\n",
    "        results = json.loads(response.text.encode())\n",
    "        if response.status_code == 200:\n",
    "            print(f'Done for page number => {separated_param[\"page\"]}')\n",
    "\n",
    "\n",
    "            # Adding your parameters to each result to be able to explore afterwards\n",
    "            for i in results['articles']:\n",
    "                i['used_params'] = str(separated_param)\n",
    "\n",
    "\n",
    "            # Storing all found articles\n",
    "            all_news_articles_mp.extend(results['articles'])\n",
    "\n",
    "            # Ensuring to cover all pages by incrementing \"page\" value at each iteration\n",
    "            separated_param['page'] += 1\n",
    "            if separated_param['page'] > results['total_pages']:\n",
    "                print(\"All articles have been extracted\")\n",
    "                break\n",
    "            else:\n",
    "                print(f'Proceed extracting page number => {separated_param[\"page\"]}')\n",
    "        else:\n",
    "            print(results)\n",
    "            print(f'ERROR: API call failed for page number => {separated_param[\"page\"]}')\n",
    "            break\n",
    "\n",
    "print(f'Number of extracted articles => {str(len(all_news_articles_mp))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables\n",
    "unique_ids = []\n",
    "all_news_articles = []\n",
    "\n",
    "# Iterate on each article and check whether we saw this _id before\n",
    "for article in all_news_articles_mp:\n",
    "    if article['_id'] not in unique_ids:\n",
    "        unique_ids.append(article['_id'])\n",
    "        all_news_articles.append(article)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate CSV from Pandas table\n",
    "# Create Pandas table\n",
    "pandas_table = pd.DataFrame(all_news_articles)\n",
    "\n",
    "# Generate CSV\n",
    "pandas_table.to_csv('extracted_news_articles.csv', encoding='utf-8', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting python-Levenshtein\n",
      "  Downloading python_Levenshtein-0.21.1-py3-none-any.whl (9.4 kB)\n",
      "Collecting Levenshtein==0.21.1\n",
      "  Downloading Levenshtein-0.21.1-cp39-cp39-macosx_11_0_arm64.whl (98 kB)\n",
      "\u001b[K     |████████████████████████████████| 98 kB 4.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rapidfuzz<4.0.0,>=2.3.0\n",
      "  Downloading rapidfuzz-3.2.0-cp39-cp39-macosx_11_0_arm64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 18.2 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
      "Successfully installed Levenshtein-0.21.1 python-Levenshtein-0.21.1 rapidfuzz-3.2.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "# Function to remove approximate duplicates from a DataFrame\n",
    "def remove_approx_duplicates(df, column_name, threshold=3):\n",
    "    cleaned_df = pd.DataFrame(columns=df.columns)\n",
    "    for _, row in df.iterrows():\n",
    "        is_duplicate = False\n",
    "        for index, cleaned_row in cleaned_df.iterrows():\n",
    "            distance = Levenshtein.distance(row[column_name], cleaned_row[column_name])\n",
    "            if distance <= threshold:\n",
    "                is_duplicate = True\n",
    "                break\n",
    "        if not is_duplicate:\n",
    "            cleaned_df = cleaned_df.append(row)\n",
    "    return cleaned_df\n",
    "\n",
    "# Remove approximate duplicates based on the \"title\" column\n",
    "cleaned_df = remove_approx_duplicates(df, column_name=\"title\")\n",
    "\n",
    "# Print the cleaned DataFrame\n",
    "print(cleaned_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

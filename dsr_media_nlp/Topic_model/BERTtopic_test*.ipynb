{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting bertopic\n",
      "  Downloading bertopic-0.15.0-py2.py3-none-any.whl (143 kB)\n",
      "\u001b[K     |████████████████████████████████| 143 kB 4.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22.2.post1 in /Users/jenskoning/Library/Python/3.9/lib/python/site-packages (from bertopic) (1.2.2)\n",
      "Collecting umap-learn>=0.5.0\n",
      "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
      "\u001b[K     |████████████████████████████████| 88 kB 14.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting plotly>=4.7.0\n",
      "  Downloading plotly-5.16.1-py2.py3-none-any.whl (15.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.6 MB 16.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting hdbscan>=0.8.29\n",
      "  Downloading hdbscan-0.8.33.tar.gz (5.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.2 MB 15.8 MB/s eta 0:00:01\n",
      "\u001b[?25h\u001b[33m  WARNING: Value for prefixed-purelib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "  distutils: /private/var/folders/3p/p467z7sj6nx1km78tdbrvvq80000gn/T/pip-build-env-_jd0hyg4/normal/lib/python3.9/site-packages\n",
      "  sysconfig: /Library/Python/3.9/site-packages\u001b[0m\n",
      "\u001b[33m  WARNING: Value for prefixed-platlib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "  distutils: /private/var/folders/3p/p467z7sj6nx1km78tdbrvvq80000gn/T/pip-build-env-_jd0hyg4/normal/lib/python3.9/site-packages\n",
      "  sysconfig: /Library/Python/3.9/site-packages\u001b[0m\n",
      "\u001b[33m  WARNING: Additional context:\n",
      "  user = False\n",
      "  home = None\n",
      "  root = None\n",
      "  prefix = '/private/var/folders/3p/p467z7sj6nx1km78tdbrvvq80000gn/T/pip-build-env-_jd0hyg4/normal'\u001b[0m\n",
      "\u001b[33m  WARNING: Value for prefixed-purelib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "  distutils: /private/var/folders/3p/p467z7sj6nx1km78tdbrvvq80000gn/T/pip-build-env-_jd0hyg4/overlay/lib/python3.9/site-packages\n",
      "  sysconfig: /Library/Python/3.9/site-packages\u001b[0m\n",
      "\u001b[33m  WARNING: Value for prefixed-platlib does not match. Please report this to <https://github.com/pypa/pip/issues/10151>\n",
      "  distutils: /private/var/folders/3p/p467z7sj6nx1km78tdbrvvq80000gn/T/pip-build-env-_jd0hyg4/overlay/lib/python3.9/site-packages\n",
      "  sysconfig: /Library/Python/3.9/site-packages\u001b[0m\n",
      "\u001b[33m  WARNING: Additional context:\n",
      "  user = False\n",
      "  home = None\n",
      "  root = None\n",
      "  prefix = '/private/var/folders/3p/p467z7sj6nx1km78tdbrvvq80000gn/T/pip-build-env-_jd0hyg4/overlay'\u001b[0m\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.41.1 in /Users/jenskoning/Library/Python/3.9/lib/python/site-packages (from bertopic) (4.64.1)\n",
      "Collecting sentence-transformers>=0.4.1\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "\u001b[K     |████████████████████████████████| 85 kB 4.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20.0 in /Users/jenskoning/Library/Python/3.9/lib/python/site-packages (from bertopic) (1.23.4)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /Users/jenskoning/Library/Python/3.9/lib/python/site-packages (from bertopic) (1.5.0)\n",
      "Collecting cython<3,>=0.27\n",
      "  Using cached Cython-0.29.36-py2.py3-none-any.whl (988 kB)\n",
      "Requirement already satisfied: joblib>=1.0 in /Users/jenskoning/Library/Python/3.9/lib/python/site-packages (from hdbscan>=0.8.29->bertopic) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.0 in /Users/jenskoning/Library/Python/3.9/lib/python/site-packages (from hdbscan>=0.8.29->bertopic) (1.10.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/jenskoning/Library/Python/3.9/lib/python/site-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/jenskoning/Library/Python/3.9/lib/python/site-packages (from pandas>=1.1.5->bertopic) (2022.4)\n",
      "Collecting tenacity>=6.2.0\n",
      "  Downloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: packaging in /Users/jenskoning/Library/Python/3.9/lib/python/site-packages (from plotly>=4.7.0->bertopic) (21.3)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas>=1.1.5->bertopic) (1.15.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/jenskoning/Library/Python/3.9/lib/python/site-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.1.0)\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Downloading transformers-4.33.1-py3-none-any.whl (7.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.6 MB 75.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torch>=1.6.0\n",
      "  Downloading torch-2.0.1-cp39-none-macosx_11_0_arm64.whl (55.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 55.8 MB 3.5 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading torchvision-0.15.2-cp39-cp39-macosx_11_0_arm64.whl (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 8.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: nltk in /Users/jenskoning/Library/Python/3.9/lib/python/site-packages (from sentence-transformers>=0.4.1->bertopic) (3.8.1)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp39-cp39-macosx_11_0_arm64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 14.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "\u001b[K     |████████████████████████████████| 268 kB 24.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0.1-cp39-cp39-macosx_11_0_arm64.whl (174 kB)\n",
      "\u001b[K     |████████████████████████████████| 174 kB 2.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /Users/jenskoning/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.28.2)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2023.9.0-py3-none-any.whl (173 kB)\n",
      "\u001b[K     |████████████████████████████████| 173 kB 3.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.12.3-py3-none-any.whl (11 kB)\n",
      "Collecting typing-extensions>=3.7.4.3\n",
      "  Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/jenskoning/Library/Python/3.9/lib/python/site-packages (from packaging->plotly>=4.7.0->bertopic) (3.0.9)\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 5.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sympy\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.7 MB 7.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /Users/jenskoning/Library/Python/3.9/lib/python/site-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/jenskoning/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2022.10.31)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp39-cp39-macosx_12_0_arm64.whl (3.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.9 MB 9.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.3.3-cp39-cp39-macosx_13_0_arm64.whl (407 kB)\n",
      "\u001b[K     |████████████████████████████████| 407 kB 12.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numba>=0.49\n",
      "  Downloading numba-0.57.1-cp39-cp39-macosx_11_0_arm64.whl (2.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.5 MB 20.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pynndescent>=0.5\n",
      "  Downloading pynndescent-0.5.10.tar.gz (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 9.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting llvmlite<0.41,>=0.40.0dev0\n",
      "  Downloading llvmlite-0.40.1-cp39-cp39-macosx_11_0_arm64.whl (28.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 28.1 MB 3.9 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /Users/jenskoning/Library/Python/3.9/lib/python/site-packages (from jinja2->torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (2.1.1)\n",
      "Requirement already satisfied: click in /Users/jenskoning/Library/Python/3.9/lib/python/site-packages (from nltk->sentence-transformers>=0.4.1->bertopic) (8.1.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jenskoning/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jenskoning/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jenskoning/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/jenskoning/Library/Python/3.9/lib/python/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (1.26.15)\n",
      "Collecting mpmath>=0.19\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[K     |████████████████████████████████| 536 kB 17.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/jenskoning/Library/Python/3.9/lib/python/site-packages (from torchvision->sentence-transformers>=0.4.1->bertopic) (9.2.0)\n",
      "Building wheels for collected packages: hdbscan, sentence-transformers, umap-learn, pynndescent\n",
      "  Building wheel for hdbscan (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hdbscan: filename=hdbscan-0.8.33-cp39-cp39-macosx_10_9_universal2.whl size=1292335 sha256=b8b8544cdab18f80ffaa9834cc6941914747c6d23402959226bb1a58bae9f7fa\n",
      "  Stored in directory: /Users/jenskoning/Library/Caches/pip/wheels/28/5e/ed/5989da4cc423a222a47cbb4fde5d6c0eff4590d922e45f233c\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=98a6322411f2231e5984886d900e634ae5fc7a7190c01393f3037434e25aef7f\n",
      "  Stored in directory: /Users/jenskoning/Library/Caches/pip/wheels/71/67/06/162a3760c40d74dd40bc855d527008d26341c2b0ecf3e8e11f\n",
      "  Building wheel for umap-learn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82830 sha256=0b547111c8bc2821a299ef64e7ba3cb770125995fcbe734e33c9a0a71d433be5\n",
      "  Stored in directory: /Users/jenskoning/Library/Caches/pip/wheels/f4/3e/1c/596d0a463d17475af648688443fa4846fef624d1390339e7e9\n",
      "  Building wheel for pynndescent (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pynndescent: filename=pynndescent-0.5.10-py3-none-any.whl size=55639 sha256=52fd242981bc93e6642bcf189f9ef703fbace422fffc1bd6070ae246d5fbea7d\n",
      "  Stored in directory: /Users/jenskoning/Library/Caches/pip/wheels/12/f9/4d/ec5ad1c823c710fcc4473669fdcffc8891f4bc398c841af22e\n",
      "Successfully built hdbscan sentence-transformers umap-learn pynndescent\n",
      "Installing collected packages: typing-extensions, mpmath, sympy, pyyaml, networkx, llvmlite, fsspec, filelock, torch, tokenizers, safetensors, numba, huggingface-hub, transformers, torchvision, tenacity, sentencepiece, pynndescent, cython, umap-learn, sentence-transformers, plotly, hdbscan, bertopic\n",
      "Successfully installed bertopic-0.15.0 cython-0.29.36 filelock-3.12.3 fsspec-2023.9.0 hdbscan-0.8.33 huggingface-hub-0.16.4 llvmlite-0.40.1 mpmath-1.3.0 networkx-3.1 numba-0.57.1 plotly-5.16.1 pynndescent-0.5.10 pyyaml-6.0.1 safetensors-0.3.3 sentence-transformers-2.2.2 sentencepiece-0.1.99 sympy-1.12 tenacity-8.2.3 tokenizers-0.13.3 torch-2.0.1 torchvision-0.15.2 transformers-4.33.1 typing-extensions-4.7.1 umap-learn-0.5.3\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jenskoning/Documents/Python_projects/dsr_media_nlp/dsr_media_nlp/Topic_model\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "print(pathlib.Path().absolute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     As part of China's massive Belt and Road Initi...\n",
       "1     AdvertisementAs part of China's massive Belt a...\n",
       "2     The Chinese government introduced the Digital ...\n",
       "3     Open AccessArticle by\\n 1,* and 2,3 \\n\\n\\n1\\nM...\n",
       "4     Besides challenging the US' Big Tech dominance...\n",
       "                            ...                        \n",
       "64    The creation of a digital agency is a top poli...\n",
       "65    China's Huawei has come to dominate the 5G rol...\n",
       "66    U\\nbiquitous digital authoritarianism simmers ...\n",
       "67    China's Huawei has come to dominate the 5G rol...\n",
       "68    Advertisement\\nIn the past decade, China's gov...\n",
       "Name: summary, Length: 69, dtype: object"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = pd.read_csv(\"extracted_news_articles_minimal.csv\", delimiter=';')\n",
    "column = docs['summary']\n",
    "column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jenskoning/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Clean stop words from test data\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "cleaned_text_data = [\n",
    "    \" \".join([word for word in sentence.split() if word.lower() not in stop_words])\n",
    "    for sentence in column\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic(embedding_model=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, probs = topic_model.fit_transform(cleaned_text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>41</td>\n",
       "      <td>-1_china_digital_chinese_bri</td>\n",
       "      <td>[china, digital, chinese, bri, global, countri...</td>\n",
       "      <td>[Yair Albeck Chinese leader Xi Jinping clearly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0_china_digital_chinese_technology</td>\n",
       "      <td>[china, digital, chinese, technology, data, ds...</td>\n",
       "      <td>[AdvertisementThe Middle East North Africa reg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1_china_chinese_standards_beijing</td>\n",
       "      <td>[china, chinese, standards, beijing, internati...</td>\n",
       "      <td>[ANI | Updated: Apr 20, 2022 06:21 IST Beijing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                Name  \\\n",
       "0     -1     41        -1_china_digital_chinese_bri   \n",
       "1      0     17  0_china_digital_chinese_technology   \n",
       "2      1     11   1_china_chinese_standards_beijing   \n",
       "\n",
       "                                      Representation  \\\n",
       "0  [china, digital, chinese, bri, global, countri...   \n",
       "1  [china, digital, chinese, technology, data, ds...   \n",
       "2  [china, chinese, standards, beijing, internati...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [Yair Albeck Chinese leader Xi Jinping clearly...  \n",
       "1  [AdvertisementThe Middle East North Africa reg...  \n",
       "2  [ANI | Updated: Apr 20, 2022 06:21 IST Beijing...  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the openAI api\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
